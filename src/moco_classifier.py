import os
import time
from pathlib import Path
import gc
import numpy as np
import pandas as pd
import lightgbm as lgb
from classifier_type import ClassifierType
from logging import getLogger
from clf_util import fast_auc, eval_auc, predict_chunk
from save_log import stop_watch, get_version


class MocoClassifier():
    def __init__(self, classifier_type, params, dataset_name):
        os.environ['KMP_DUPLICATE_LIB_OK'] = "True"
        self.classifier_type = classifier_type
        self.params = params
        self.dataset_name = dataset_name

    @stop_watch("MocoClassifier.train()")
    def train(self, feature_names):
        clfs = self.__train_as_classifier_type(feature_names)
        self.clfs = clfs

    @stop_watch("MocoClassifier.predict()")
    def predict(self, feature_names):
        assert False
        for fold in range(5):
            valid = "valid{}".format(fold)
            test = self.__get_feature_df(self.dataset_name, feature_names, valid, "test")
            del test["MachineIdentifier"]

    def __get_feature_df(self, dataset_name, feature_names, valid_dir, part):
        """
        Ex)
        dataset_name    : min, ...
        feature_names   : __preprocess() return
        valid_dir       : "valid0", "valid1", ...
        part            : "train", "validate", "test"
        """
        feature_df = None
        featureset_path = Path(__file__).absolute().parents[1] / "data" / "features" / dataset_name / valid_dir
        for group, feature_list in feature_names.items():
            df = pd.read_csv(featureset_path / "{}_{}.csv".format(part, group),
                             usecols=["MachineIdentifier"] + feature_list)
            if feature_df is None:
                feature_df = df
            else:
                feature_df = feature_df.merge(right=df,
                                              how="inner",
                                              on="MachineIdentifier")
        if part in ["train", "validate"]:
            HasDetections = pd.read_csv(Path(__file__).absolute().parents[1] / "input" / "train.csv",
                                        usecols=["MachineIdentifier", "HasDetections"])
            feature_df = feature_df.merge(right=HasDetections,
                                          on="MachineIdentifier",
                                          how="inner")
        return feature_df

    def __train_as_classifier_type(self, feature_names):
        if self.classifier_type == ClassifierType.LGBM:
            return self.__train_with_lgbm(feature_names)
        elif self.classifier_type == ClassifierType.CAT_BOOST:
            return self.__train_with_cat_boost(feature_names)

    def __train_with_lgbm(self, feature_names):
        clfs = []
        mtd_params = self.params["mtd_params"]
        trn_params = self.params["trn_params"]
        lgb_params = {
            "objective": "binary",
            "boosting": trn_params["boosting"],
            "metric": trn_params["metric"],
            "n_estimators": trn_params["n_estimators"],
            "learning_rate": trn_params["learning_rate"],
            "num_leaves": trn_params["num_leaves"],
            "n_jobs": -1,
            "seed": 1116,
            "max_depth": trn_params["max_depth"],
            "min_child_samples": trn_params["min_child_samples"],
            "verbosity": -1
        }
        oof = None
        y_true = None
        feature_importance = pd.DataFrame()
        for fold in range(5):
            start = time.time()
            getLogger(get_version()).info("\t >> {} folds start".format(fold))
            valid = "valid{}".format(fold)

            # Generate dataset
            trn_x = self.__get_feature_df(self.dataset_name, feature_names, valid, "train")
            trn_y = trn_x["HasDetections"].astype(np.int8)
            del trn_x["MachineIdentifier"], trn_x["HasDetections"]
            train_dataset = lgb.Dataset(trn_x, trn_y)

            val_x = self.__get_feature_df(self.dataset_name, feature_names, valid, "validate")
            val_y = val_x["HasDetections"].astype(np.int8)
            del val_x["MachineIdentifier"], val_x["HasDetections"]
            valid_dataset = lgb.Dataset(val_x, val_y)

            if oof is None:
                oof = np.zeros(len(trn_x)+len(val_x))
            if y_true is None:
                y_true = pd.concat([trn_y, val_y])

            # Classify
            clf = lgb.train(lgb_params,
                            train_dataset,
                            mtd_params["num_boost_round"],
                            valid_sets=[train_dataset, valid_dataset],
                            verbose_eval=mtd_params["verbose_eval"],
                            early_stopping_rounds=mtd_params["early_stopping_rounds"])
            del train_dataset, valid_dataset

            oof[val_x.index] = clf.predict(val_x, num_iteration=clf.best_iteration)

            # Calculate feature importance per fold
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = trn_x.columns
            fold_importance["importance"] = clf.feature_importance(importance_type="gain")
            fold_importance["fold"] = fold
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

            elapsed_time = int(time.time() - start)
            minits, sec = divmod(elapsed_time, 60)
            hour, minits = divmod(minits, 60)
            getLogger(get_version()).info(
                "\t >> {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}"
                .format(fold, hour, minits, sec))
            clfs.append(clf)
            gc.collect()

        # Output CV score
        cv_auc = (fast_auc(y_true, oof))
        getLogger(get_version()).info(
            "\t >> CV Score (AUC):{}".format(cv_auc))

        # Save importance
        mean_importance = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(by="importance", ascending=False)
        directory_path = Path(__file__).absolute().parents[1] / "importance"
        Path.mkdir(directory_path, exist_ok=True, parents=True)
        mean_importance.to_csv(Path(directory_path / "{}.csv".format(get_version())))

        return clfs

    def __train_with_cat_boost(self, df):
        clfs = "Please define cat boost functions"
        return clfs
