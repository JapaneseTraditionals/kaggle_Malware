import os
from pathlib import Path
import gc
import numpy as np
import pandas as pd
import lightgbm as lgb
from classifier_type import ClassifierType
from logging import getLogger
from clf_util import fast_auc, eval_auc, predict_chunk
from save_log import stop_watch, get_version


class MocoClassifier():
    def __init__(self, classifier_type, params, dataset_name):
        os.environ['KMP_DUPLICATE_LIB_OK'] = "True"
        self.classifier_type = classifier_type
        self.params = params
        self.dataset_name = dataset_name

    @stop_watch("MocoClassifier.train()")
    def train(self, train_df):
        clfs = self.__train_as_classifier_type(train_df)
        importances = "Please calculate importances of features"
        self.__save_importances(importances)
        self.clfs = clfs

    @stop_watch("MocoClassifier.predict()")
    def predict(self, test_df):
        if self.classifier_type == ClassifierType.LGBM:
            return self.__predict_with_lgbm(test_df)
        elif self.classifier_type == ClassifierType.CAT_BOOST:
            return self.__predict_with_cat_boost(test_df)

    def __get_feature_df(self, dataset_name, feature_names, valid_dir, part):
        """
        Ex)
        dataset_name    : min, ...
        feature_names   : __preprocess() return
        valid_dir       : "valid0", "valid1", ...
        part            : "train", "validate", "test"
        """
        feature_df = None
        featureset_path = Path(__file__).absolute().parents[1] / "data" / "features" / dataset_name / valid_dir
        for group, feature_list in feature_names.items():
            df = pd.read_csv(featureset_path / "{}_{}.csv".format(part, group),
                             usecols=["MachineIdentifier"] + feature_list)
            if feature_df is None:
                feature_df = df
            else:
                feature_df = feature_df.merge(right=df,
                                              how="inner",
                                              on="MachineIdentifier")
        return feature_df

    def __train_as_classifier_type(self, feature_names):
        if self.classifier_type == ClassifierType.LGBM:
            return self.__train_with_lgbm(feature_names)
        elif self.classifier_type == ClassifierType.CAT_BOOST:
            return self.__train_with_cat_boost(feature_names)

    def __train_with_lgbm(self, feature_names):
        clfs = []
        mtd_params = self.params["mtd_params"]
        trn_params = self.params["trn_params"]
        lgb_params = {
            "objective": "binary",
            "boosting": trn_params["boosting"],
            "metric": trn_params["metric"],
            "n_estimators": trn_params["n_estimators"],
            "learning_rate": trn_params["learning_rate"],
            "num_leaves": trn_params["num_leaves"],
            "n_jobs": -1,
            "seed": 1116,
            "max_depth": trn_params["max_depth"],
            "min_child_samples": trn_params["min_child_samples"],
            "verbosity": -1
        }
        # oof_preds = np.zeros((len(df), np.unique(y).shape[0]))
        for fold in range(5):
            getLogger(get_version()).info("\t >> {} folds start".format(fold))
            valid = "valid{}".format(fold)
            trn_x = self.__get_feature_df(self.dataset_name, feature_names, valid, "train")
            print(trn_x.head())
            trn_y = trn_x["HasDetections"].astype(np.int8)
            del trn_x["MachineIdentifier"], trn_x["HasDetections"], trn_y["MachineIdentifier"]
            train_dataset = lgb.Dataset(trn_x, trn_y)
            val_x = self.__get_feature_df(self.dataset_name, feature_names, valid, "validate")
            val_y = val_x["HasDetections"].astype(np.int8)
            del val_x["MachineIdentifier"], val_x["HasDetections"], val_y["MachineIdentifier"]
            valid_dataset = lgb.Dataset(val_x, val_y)
            clf = lgb.train(lgb_params,
                            train_dataset,
                            mtd_params["num_boost_round"],
                            valid_sets=valid_dataset,
                            verbose_eval=mtd_params["verbose_eval"],
                            early_stopping_rounds=mtd_params["early_stopping_rounds"])
            del train_dataset, valid_dataset
            print(clf)

        assert False

        return clfs

    def __predict_with_lgbm(self, df):
        submit = "Please predict df with self.clf"
        return submit

    def __train_with_cat_boost(self, df):
        clf = "Please define cat boost functions"
        return clf

    def __predict_with_cat_boost(self, df):
        submit = "Please predict df with self.clf"
        return submit

    def __save_importances(self, importances):
        """
        # TODO: Implementation of CSV
        mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()
        importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])
        plt.figure(figsize=(16, 24))
        sns.barplot(x='gain', y='feature',
                    data=importances_.sort_values('mean_gain', ascending=False))
        plt.tight_layout()
        plt.savefig(Path(__file__).absolute().parents[3] / "data" /
                    "importances" / "{}.png".format(version))
        """
        pass
