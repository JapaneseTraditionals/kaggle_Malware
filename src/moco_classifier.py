import os
import time
from pathlib import Path
import gc
import numpy as np
import pandas as pd
import lightgbm as lgb
from classifier_type import ClassifierType
from logging import getLogger
from clf_util import fast_auc, eval_auc, predict_chunk
from save_log import stop_watch, get_version


class MocoClassifier():
    def __init__(self, classifier_type, params, dataset_name):
        os.environ['KMP_DUPLICATE_LIB_OK'] = "True"
        self.classifier_type = classifier_type
        self.params = params
        self.dataset_name = dataset_name

    @stop_watch("MocoClassifier.train()")
    def train(self, feature_names):
        clfs, validity = self.__train_as_classifier_type(feature_names)
        self.clfs = clfs
        return validity

    @stop_watch("MocoClassifier.predict()")
    def predict(self, feature_names):
        assert False
        for fold in range(5):
            valid = "valid{}".format(fold)
            test = self.__get_feature_df(self.dataset_name, feature_names, valid, "test")
            del test["MachineIdentifier"]

    def __get_feature_df(self, dataset_name, feature_names, valid_dir, part):
        """
        Ex)
        dataset_name    : min, ...
        feature_names   : __preprocess() return
        valid_dir       : "valid0", "valid1", ...
        part            : "train", "validate", "test"
        """
        feature_df = None
        featureset_path = Path(__file__).absolute().parents[1] / "data" / "features" / dataset_name / valid_dir
        for group, feature_list in feature_names.items():
            df = pd.read_csv(featureset_path / "{}_{}.csv".format(part, group),
                             usecols=["MachineIdentifier"] + feature_list)
            if feature_df is None:
                feature_df = df
            else:
                feature_df = feature_df.merge(right=df,
                                              how="inner",
                                              on="MachineIdentifier")
        if part in ["train", "validate"]:
            HasDetections = pd.read_csv(Path(__file__).absolute().parents[1] / "input" / "train.csv",
                                        usecols=["MachineIdentifier", "HasDetections"])
            feature_df = feature_df.merge(right=HasDetections,
                                          on="MachineIdentifier",
                                          how="inner")
        return feature_df

    def __train_as_classifier_type(self, feature_names):
        if self.classifier_type == ClassifierType.LGBM:
            return self.__train_with_lgbm(feature_names)
        elif self.classifier_type == ClassifierType.CAT_BOOST:
            return self.__train_with_cat_boost(feature_names)

    def __train_with_lgbm(self, feature_names):
        """
        FLow:
            1. Initialize parameters
            2. Process for each fold
                2.1 Generate dataset
                2.2 Classify
                2.3 calculate feature importances
            3. Output CV Score and features importances
            4. Predict training data (validate all data)
        Input:
            feature_names: directionary of features' names
        Output:
            clfs: list of classifiers per folds
            validity: Dataframe(["MachineIdentifier", "HasDetections", "Predict")
        """

        # Initialize parameters
        clfs = []
        mtd_params = self.params["mtd_params"]
        trn_params = self.params["trn_params"]
        lgb_params = {
            "objective": "binary",
            "boosting": trn_params["boosting"],
            "metric": trn_params["metric"],
            "n_estimators": trn_params["n_estimators"],
            "learning_rate": trn_params["learning_rate"],
            "num_leaves": trn_params["num_leaves"],
            "n_jobs": -1,
            "seed": 1116,
            "max_depth": trn_params["max_depth"],
            "min_child_samples": trn_params["min_child_samples"],
            "verbosity": -1
        }
        oof = None
        y_true = None
        machine_identifier = None
        feature_importance = pd.DataFrame()

        # Process for each fold
        for fold in range(5):
            # Measure start time of the classification of this fold
            start = time.time()
            getLogger(get_version()).info("\t >> {} folds start".format(fold))

            # Generate dataset
            valid = "valid{}".format(fold)
            trn_x = self.__get_feature_df(self.dataset_name, feature_names, valid, "train")
            val_x = self.__get_feature_df(self.dataset_name, feature_names, valid, "validate")
            trn_y = trn_x["HasDetections"].astype(np.int8)
            val_y = val_x["HasDetections"].astype(np.int8)
            train_dataset = lgb.Dataset(trn_x, trn_y)
            valid_dataset = lgb.Dataset(val_x, val_y)

            # Initialize variables for scoring
            if oof is None:
                oof = np.zeros(len(trn_x)+len(val_x))
            if y_true is None:
                y_true = pd.concat([trn_y, val_y])
            if machine_identifier is None:
                machine_identifier = pd.concat([trn_x["MachineIdentifier"], val_x["MachineIdentifier"]])

            # Delete needless features
            del trn_x["MachineIdentifier"], trn_x["HasDetections"]
            del val_x["MachineIdentifier"], val_x["HasDetections"]

            # Classify
            clf = lgb.train(lgb_params,
                            train_dataset,
                            mtd_params["num_boost_round"],
                            valid_sets=[train_dataset, valid_dataset],
                            verbose_eval=mtd_params["verbose_eval"],
                            early_stopping_rounds=mtd_params["early_stopping_rounds"])
            oof[val_x.index] = clf.predict(val_x, num_iteration=clf.best_iteration)

            # Calculate feature importance per fold
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = trn_x.columns
            fold_importance["importance"] = clf.feature_importance(importance_type="gain")
            fold_importance["fold"] = fold
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

            # Measure finish time of the classification of this fold
            elapsed_time = int(time.time() - start)
            minits, sec = divmod(elapsed_time, 60)
            hour, minits = divmod(minits, 60)
            getLogger(get_version()).info(
                "\t >> {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}"
                .format(fold, hour, minits, sec))

            # Post-process this fold
            del train_dataset, valid_dataset, fold_importance
            gc.collect()
            clfs.append(clf)

        # Output CV score
        # TODO Please check here
        cv_auc = (fast_auc(y_true, oof))
        getLogger(get_version()).info(
            "\t >> CV Score (AUC):{}".format(cv_auc))

        # Save importance
        mean_importance = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(by="importance", ascending=False)
        directory_path = Path(__file__).absolute().parents[1] / "importance"
        Path.mkdir(directory_path, exist_ok=True, parents=True)
        mean_importance.to_csv(Path(directory_path / "{}.csv".format(get_version())))

        # Predict training data
        # TODO naosu aspe
        validity = y_true

        # Post-process the training
        del oof, y_true, machine_identifier, feature_importance, mean_importance
        gc.collect()

        return clfs, validity

    def __train_with_cat_boost(self, df):
        clfs = "Please define cat boost functions"
        validity = "Please calc predict of training"
        return clfs, validity
