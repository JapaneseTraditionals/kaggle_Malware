import gc
import time
import copy
from pathlib import Path
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from logging import getLogger
from clf_util import fast_auc
from classifier.base_classifier import BaseClassifier
from classifier.pytorch_network.simple_network import SimpleNet
from save_log import stop_watch, get_back_training, train_one_round, get_version, send_message


class PyTorchClassifier(BaseClassifier):

    @stop_watch("PyTorchClassifier.train()")
    def train(self, feature_names):

        # Initialize parameters
        clfs = []
        validity = None
        model_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        Path.mkdir(model_path, exist_ok=True, parents=True)
        feature_importance = pd.DataFrame()
        START_FOLD = 0
        if get_back_training():
            START_FOLD = len(list(model_path.glob('**/*.model')))
        END_FOLD = 5
        if train_one_round():
            START_FOLD = 0
            END_FOLD = 1
        if START_FOLD == END_FOLD:
            return None

        # Process for each fold
        for fold in range(START_FOLD, END_FOLD):
            # Measure start time of the classification of this fold
            start = time.time()
            getLogger(get_version()).info("\t >> {} folds start".format(fold))
            send_message("\t :flashlight: {} folds start".format(fold))
            valid = "valid{}".format(str(fold))

            # Generate train dataset
            getLogger(get_version()).info("\t \t Generating train datasets...")
            send_message("\t \t Generating train datasets...")
            trn_x = super().get_feature_df(feature_names, valid, "train")
            trn_x.set_index("MachineIdentifier", inplace=True)
            trn_y = trn_x["HasDetections"].values.astype(np.float32)
            del trn_x["HasDetections"]
            trn_x_tensor = torch.tensor(trn_x.values.astype(np.float32))
            trn_y_tensor = torch.tensor(trn_y)
            trn_dataset = TensorDataset(trn_x_tensor, trn_y_tensor)
            train_loader = DataLoader(dataset=trn_dataset, batch_size=self.params["batch_size"], shuffle=True)

            # Generate valid dataset
            getLogger(get_version()).info("\t \t Generating valid datasets...")
            send_message("\t \t Generating valid datasets...")
            val_x = super().get_feature_df(feature_names, valid, "validate")
            val_x.set_index("MachineIdentifier", inplace=True)
            val_y = val_x["HasDetections"].values.astype(np.float32)
            del val_x["HasDetections"]
            val_x_tensor = torch.tensor(val_x.values.astype(np.float32))
            val_y_tensor = torch.tensor(val_y)
            val_dataset = TensorDataset(val_x_tensor, val_y_tensor)
            valid_loader = DataLoader(dataset=val_dataset, batch_size=self.params["batch_size"], shuffle=False)

            # Create data_loaders
            data_loaders = {"train": train_loader, "valid": valid_loader}
            getLogger(get_version()).info("\t \t Datasets were generated.")
            send_message("\t \t Datasets were generated.")

            # Initialize variables for scoring
            """
            if validity is None:
                validity = pd.DataFrame()
                validity["HasDetections"] = pd.concat([trn_y, val_y])
                validity["Predict"] = 0
            """

            # Define the Network
            net = SimpleNet(len(trn_x.columns))
            criterion = nn.BCELoss()
            optimizer = optim.Adam(net.parameters(), lr=0.01)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)
            num_epochs = 32
            self.__train_model(data_loaders,
                               net,
                               criterion,
                               optimizer,
                               scheduler,
                               num_epochs=num_epochs)

            assert False

            # Classify
            """
            validity.loc[validity.index.isin(val_x.index), "Predict"] = clf.predict(val_x, num_iteration=clf.best_iteration)
            for train_or_valid, metrics in clf.best_score.items():
                for metric, score in metrics.items():
                    getLogger(get_version()).info("\t\t >> Best {} {}: {}".format(train_or_valid, metric, score))
                    send_message("\t\t :star-struck: Best {} {}: {}".format(train_or_valid, metric, score))
            """

            # Calculate feature importance per fold
            """
            if fold == 0:
                feature_importance["feature"] = trn_x.columns
            feature_importance["fold{}".format(fold)] = clf.feature_importance(importance_type="gain")
            """

            # Measure finish time of the classification of this fold
            elapsed_time = int(time.time() - start)
            minutes, sec = divmod(elapsed_time, 60)
            hour, minutes = divmod(minutes, 60)
            getLogger(get_version()).info(
                "\t >> {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}"
                .format(fold, hour, minutes, sec))
            send_message("\t :flashlight: {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}".format(fold, hour, minutes, sec))

            # Post-process this fold
            gc.collect()
            # clfs.append(clf)
            # clf.save_model(str(model_path / "valid{}.model".format(fold)))

        # Output CV score
        validity = validity.reset_index()
        columns_order = ["MachineIdentifier", "HasDetections", "Predict"]
        validity = validity.sort_values("MachineIdentifier").reset_index(drop=True).loc[:, columns_order]
        cv_auc = (fast_auc(validity["HasDetections"], np.array(validity["Predict"])))
        getLogger(get_version()).info("\t >> CV Score (AUC):{}".format(cv_auc))
        send_message("\t :flashlight: CV Score (AUC):{}".format(cv_auc))

        # Save importance
        feature_importance.set_index("feature", inplace=True)
        feature_importance["median"] = feature_importance.median(axis='columns')
        feature_importance.sort_values("median", ascending=False, inplace=True)
        directory_path = Path(__file__).absolute().parents[2] / "importance"
        Path.mkdir(directory_path, exist_ok=True, parents=True)
        feature_importance.to_csv(Path(directory_path / "{}.csv".format(get_version())))

        # Post-process the training
        del feature_importance
        gc.collect()

        return validity

    @stop_watch("LGBMClassifier.predict()")
    def predict(self, feature_names):
        """
        Input:
            feature_names: directionary of features' names
        Output:
            predict_df: Dataframe(["MachineIdentifier", "HasDetections")
        """
        # model_directory_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        preds = None
        FOLDS = 5
        predict_df = None
        for fold in range(FOLDS):
            # model_path = model_directory_path / "valid{}.model".format(fold)
            # clf = lgb.Booster(model_file=str(model_path))
            valid = "valid{}".format(fold)
            test_df = super().get_feature_df(feature_names, valid, "test")
            if predict_df is None:
                predict_df = test_df["MachineIdentifier"]
            test_df = test_df.set_index("MachineIdentifier")
            """
            if preds is None:
                preds = predict_chunk(clf, test_df) / FOLDS
            else:
                preds += predict_chunk(clf, test_df) / FOLDS
            """

        predict_df = pd.DataFrame(predict_df)
        predict_df["HasDetections"] = preds
        return predict_df

    def __train_model(self, data_loaders, model, criterion, optimizer, scheduler, num_epochs=25):
        since = time.time()

        best_model_wts = copy.deepcopy(model.state_dict())
        best_log_loss = 1.0
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        for epoch in range(num_epochs):
            print("Epoch {}/{}".format(epoch, num_epochs - 1))
            print("-" * 10)
            for phase in ["train", "valid"]:
                if phase == "train":
                    scheduler.step()
                    model.train()
                else:
                    model.eval()

                running_loss = 0.0
                running_corrects = 0

                # Iteration
                for inputs, labels in data_loaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    optimizer.zero_grad()

                    # forward
                    with torch.set_grad_enabled(phase == "train"):
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                        # backward + optimize
                        if phase == "train":
                            loss.backward()
                            optimizer.step()

                    # statistics
                    running_loss += loss.item() * inputs.size(0)
                    print(outputs)
                    print(labels.data)
                    running_corrects += torch.sum(outputs == labels.data)

        return model
