import gc
import time
from pathlib import Path
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from logging import getLogger
from clf_util import fast_auc
from classifier.base_classifier import BaseClassifier
from classifier.torch_util import create_tensor_dataloader, train_model
from classifier.pytorch_network.simple_network import SimpleNet
from save_log import stop_watch, get_back_training, train_one_round, get_version, send_message


class PyTorchClassifier(BaseClassifier):

    @stop_watch("PyTorchClassifier.train()")
    def train(self, feature_names):

        # Initialize parameters
        clfs = []
        validity = None
        model_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        Path.mkdir(model_path, exist_ok=True, parents=True)
        feature_importance = pd.DataFrame()
        START_FOLD = 0
        if get_back_training():
            START_FOLD = len(list(model_path.glob('**/*.model')))
        END_FOLD = 5
        if train_one_round():
            START_FOLD = 0
            END_FOLD = 1
        if START_FOLD == END_FOLD:
            return None

        # Process for each fold
        for fold in range(START_FOLD, END_FOLD):
            # Measure start time of the classification of this fold
            start = time.time()
            getLogger(get_version()).info("\t >> {} folds start".format(fold))
            send_message("\t :flashlight: {} folds start".format(fold))
            valid = "valid{}".format(str(fold))

            # Generate train data
            getLogger(get_version()).info("\t \t Generating datasets...")
            send_message("\t \t Generating datasets...")

            trn_x = super().get_feature_df(feature_names, valid, "train")
            trn_x.set_index("MachineIdentifier", inplace=True)
            trn_y = trn_x["HasDetections"].values.astype(np.float32)

            val_x = super().get_feature_df(feature_names, valid, "validate")
            val_x.set_index("MachineIdentifier", inplace=True)
            val_y = val_x["HasDetections"].values.astype(np.float32)

            del trn_x["HasDetections"], val_x["HasDetections"]

            train_loader, valid_loader, dataset_sizes = create_tensor_dataloader(self.params, trn_x, trn_y, val_x, val_y)
            data_loaders = {"train": train_loader, "valid": valid_loader}

            getLogger(get_version()).info("\t \t Datasets were generated.")
            send_message("\t \t Datasets were generated.")

            # Initialize variables for scoring
            """
            if validity is None:
                validity = pd.DataFrame()
                validity["HasDetections"] = pd.concat([trn_y, val_y])
                validity["Predict"] = 0
            """

            # Define the Network
            net = SimpleNet(len(trn_x.columns))
            criterion = nn.BCELoss()    # Don't change the criterion function!
            optimizer = optim.Adam(net.parameters(), lr=0.01)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)
            num_epochs = 32
            train_model(data_loaders, dataset_sizes, net, criterion,
                        optimizer, scheduler, num_epochs=num_epochs)

            assert False

            # Classify
            """
            validity.loc[validity.index.isin(val_x.index), "Predict"] = clf.predict(val_x, num_iteration=clf.best_iteration)
            for train_or_valid, metrics in clf.best_score.items():
                for metric, score in metrics.items():
                    getLogger(get_version()).info("\t\t >> Best {} {}: {}".format(train_or_valid, metric, score))
                    send_message("\t\t :star-struck: Best {} {}: {}".format(train_or_valid, metric, score))
            """

            # Calculate feature importance per fold
            """
            if fold == 0:
                feature_importance["feature"] = trn_x.columns
            feature_importance["fold{}".format(fold)] = clf.feature_importance(importance_type="gain")
            """

            # Measure finish time of the classification of this fold
            elapsed_time = int(time.time() - start)
            minutes, sec = divmod(elapsed_time, 60)
            hour, minutes = divmod(minutes, 60)
            getLogger(get_version()).info(
                "\t >> {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}"
                .format(fold, hour, minutes, sec))
            send_message("\t :flashlight: {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}".format(fold, hour, minutes, sec))

            # Post-process this fold
            gc.collect()
            # clfs.append(clf)
            # clf.save_model(str(model_path / "valid{}.model".format(fold)))

        # Output CV score
        validity = validity.reset_index()
        columns_order = ["MachineIdentifier", "HasDetections", "Predict"]
        validity = validity.sort_values("MachineIdentifier").reset_index(drop=True).loc[:, columns_order]
        cv_auc = (fast_auc(validity["HasDetections"], np.array(validity["Predict"])))
        getLogger(get_version()).info("\t >> CV Score (AUC):{}".format(cv_auc))
        send_message("\t :flashlight: CV Score (AUC):{}".format(cv_auc))

        # Save importance
        feature_importance.set_index("feature", inplace=True)
        feature_importance["median"] = feature_importance.median(axis='columns')
        feature_importance.sort_values("median", ascending=False, inplace=True)
        directory_path = Path(__file__).absolute().parents[2] / "importance"
        Path.mkdir(directory_path, exist_ok=True, parents=True)
        feature_importance.to_csv(Path(directory_path / "{}.csv".format(get_version())))

        # Post-process the training
        del feature_importance
        gc.collect()

        return validity

    @stop_watch("LGBMClassifier.predict()")
    def predict(self, feature_names):
        """
        Input:
            feature_names: directionary of features' names
        Output:
            predict_df: Dataframe(["MachineIdentifier", "HasDetections")
        """
        # model_directory_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        preds = None
        FOLDS = 5
        predict_df = None
        for fold in range(FOLDS):
            # model_path = model_directory_path / "valid{}.model".format(fold)
            # clf = lgb.Booster(model_file=str(model_path))
            valid = "valid{}".format(fold)
            test_df = super().get_feature_df(feature_names, valid, "test")
            if predict_df is None:
                predict_df = test_df["MachineIdentifier"]
            test_df = test_df.set_index("MachineIdentifier")
            """
            if preds is None:
                preds = predict_chunk(clf, test_df) / FOLDS
            else:
                preds += predict_chunk(clf, test_df) / FOLDS
            """

        predict_df = pd.DataFrame(predict_df)
        predict_df["HasDetections"] = preds
        return predict_df
