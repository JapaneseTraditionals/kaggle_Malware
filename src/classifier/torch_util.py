import time
import copy
import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset


def create_tensor_dataloader(params, trn_x, trn_y, val_x, val_y):
    # Generate train_loader
    trn_x_tensor = torch.tensor(trn_x.values.astype(np.float32))
    trn_y_tensor = torch.tensor(trn_y)
    trn_dataset = TensorDataset(trn_x_tensor, trn_y_tensor)
    train_loader = DataLoader(dataset=trn_dataset, batch_size=params["batch_size"], shuffle=True)

    # Generate valid_loader
    val_x_tensor = torch.tensor(val_x.values.astype(np.float32))
    val_y_tensor = torch.tensor(val_y)
    val_dataset = TensorDataset(val_x_tensor, val_y_tensor)
    valid_loader = DataLoader(dataset=val_dataset, batch_size=params["batch_size"], shuffle=False)

    return train_loader, valid_loader


def train_model(data_loaders, model, criterion, optimizer, scheduler, num_epochs=25, verbose=10):
        since = time.time()

        best_model_wts = copy.deepcopy(model.state_dict())
        best_log_loss = 1.0
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        for epoch in range(num_epochs):
            print("Epoch {}/{}".format(epoch, num_epochs - 1))
            print("-" * 10)
            for phase in ["train", "valid"]:
                if phase == "train":
                    scheduler.step()
                    model.train()
                else:
                    model.eval()

                running_loss = 0.0

                # Iteration
                for inputs, labels in data_loaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    optimizer.zero_grad()

                    # forward
                    with torch.set_grad_enabled(phase == "train"):
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                        # backward + optimize
                        if phase == "train":
                            loss.backward()
                            optimizer.step()

                    # statistics
                    running_loss += loss.item() * inputs.size(0)
                print('Epoch {}/{} \t loss={:.4f}'.format(
                    epoch + 1, num_epochs, running_loss))

        return model
