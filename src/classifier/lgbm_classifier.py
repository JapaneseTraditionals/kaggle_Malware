import gc
import time
from pathlib import Path
import numpy as np
import pandas as pd
import lightgbm as lgb
import logging
from logging import getLogger
from lightgbm.callback import _format_eval_result
from save_log import get_version, send_message
from clf_util import fast_auc, eval_auc, predict_chunk
from classifier.base_classifier import BaseClassifier
from save_log import stop_watch, get_back_training, get_training_logger, train_one_round


class LGBMClassifier(BaseClassifier):

    @stop_watch("LGBMClassifier.train()")
    def train(self, feature_names):
        """
        FLow:
            1. Initialize parameters
            2. Process for each fold
                2.1 Generate dataset
                2.2 Classify
                2.3 calculate feature importances
            3. Output CV Score and features importances
            4. Predict training data (validate all data)
        Input:
            feature_names: directionary of features' names
        Output:
            validity: Dataframe(["MachineIdentifier", "HasDetections", "Predict")
        """

        # Initialize parameters
        clfs = []
        mtd_params = self.params["mtd_params"]
        validity = None
        model_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        Path.mkdir(model_path, exist_ok=True, parents=True)
        feature_importance = pd.DataFrame()
        START_FOLD = 0
        if get_back_training():
            START_FOLD = len(list(model_path.glob('**/*.model')))
        END_FOLD = 5
        if train_one_round():
            START_FOLD = 0
            END_FOLD = 1
        if START_FOLD == END_FOLD:
            return None
        # Process for each fold
        for fold in range(START_FOLD, END_FOLD):
            # Measure start time of the classification of this fold
            start = time.time()
            getLogger(get_version()).info("\t >> {} folds start".format(fold))
            send_message("\t :flashlight: {} folds start".format(fold))

            # Generate dataset
            valid = "valid{}".format(str(fold))
            trn_x = super().get_feature_df(feature_names, valid, "train")
            val_x = super().get_feature_df(feature_names, valid, "validate")
            trn_x.set_index("MachineIdentifier", inplace=True)
            val_x.set_index("MachineIdentifier", inplace=True)
            trn_y = trn_x["HasDetections"].astype(np.int8)
            val_y = val_x["HasDetections"].astype(np.int8)
            train_dataset = lgb.Dataset(trn_x, trn_y)
            valid_dataset = lgb.Dataset(val_x, val_y)

            # Initialize variables for scoring
            if validity is None:
                validity = pd.DataFrame()
                validity["HasDetections"] = pd.concat([trn_y, val_y])
                validity["Predict"] = 0

            # Delete needless features
            del trn_x["HasDetections"], val_x["HasDetections"]

            # Classify
            callbacks = [log_evaluation(get_training_logger(get_version()), fold)]
            clf = lgb.train(self.params["trn_params"],
                            train_dataset,
                            mtd_params["num_boost_round"],
                            valid_sets=[train_dataset, valid_dataset],
                            feval=eval_auc,
                            verbose_eval=mtd_params["verbose_eval"],
                            early_stopping_rounds=mtd_params["early_stopping_rounds"],
                            callbacks=callbacks)
            validity.loc[validity.index.isin(val_x.index), "Predict"] = clf.predict(val_x, num_iteration=clf.best_iteration)
            for train_or_valid, metrics in clf.best_score.items():
                for metric, score in metrics.items():
                    getLogger(get_version()).info("\t >> Best {} {}: {}".format(train_or_valid, metric, score))
                    send_message("\t :star-struck: Best {} {}: {}".format(train_or_valid, metric, score))

            # Calculate feature importance per fold
            if fold == 0:
                feature_importance["feature"] = trn_x.columns
            feature_importance["fold{}".format(fold)] = clf.feature_importance(importance_type="gain")

            # Measure finish time of the classification of this fold
            elapsed_time = int(time.time() - start)
            minutes, sec = divmod(elapsed_time, 60)
            hour, minutes = divmod(minutes, 60)
            getLogger(get_version()).info(
                "\t >> {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}"
                .format(fold, hour, minutes, sec))
            send_message("\t :flashlight: {} folds finish: [elapsed_time] >> {:0>2}:{:0>2}:{:0>2}".format(fold, hour, minutes, sec))

            # Post-process this fold
            del train_dataset, valid_dataset
            gc.collect()
            clfs.append(clf)
            clf.save_model(str(model_path / "valid{}.model".format(fold)))

        # Output CV score
        validity = validity.reset_index()
        columns_order = ["MachineIdentifier", "HasDetections", "Predict"]
        validity = validity.sort_values("MachineIdentifier").reset_index(drop=True).loc[:, columns_order]
        cv_auc = (fast_auc(validity["HasDetections"], np.array(validity["Predict"])))
        getLogger(get_version()).info("\t >> CV Score (AUC):{}".format(cv_auc))
        send_message("\t :flashlight: CV Score (AUC):{}".format(cv_auc))

        # Save importance
        feature_importance.set_index("feature", inplace=True)
        feature_importance["median"] = feature_importance.median(axis='columns')
        feature_importance.sort_values("median", ascending=False, inplace=True)
        directory_path = Path(__file__).absolute().parents[2] / "importance"
        Path.mkdir(directory_path, exist_ok=True, parents=True)
        feature_importance.to_csv(Path(directory_path / "{}.csv".format(get_version())))

        # Post-process the training
        del feature_importance
        gc.collect()

        return validity

    @stop_watch("LGBMClassifier.predict()")
    def predict(self, feature_names):
        """
        Input:
            feature_names: directionary of features' names
        Output:
            predict_df: Dataframe(["MachineIdentifier", "HasDetections")
        """
        model_directory_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        preds = None
        FOLDS = 5
        predict_df = None
        for fold in range(FOLDS):
            model_path = model_directory_path / "valid{}.model".format(fold)
            clf = lgb.Booster(model_file=str(model_path))
            valid = "valid{}".format(fold)
            test_df = super().get_feature_df(feature_names, valid, "test")
            if predict_df is None:
                predict_df = test_df["MachineIdentifier"]
            test_df = test_df.set_index("MachineIdentifier")
            if preds is None:
                preds = predict_chunk(clf, test_df) / FOLDS
            else:
                preds += predict_chunk(clf, test_df) / FOLDS

        predict_df = pd.DataFrame(predict_df)
        predict_df["HasDetections"] = preds
        return predict_df


def log_evaluation(logger, fold, period=1, show_stdv=True, level=logging.DEBUG):
    def _callback(env):
        if period > 0 and env.evaluation_result_list and (env.iteration + 1) % period == 0:
            result_list = [_format_eval_result(x, show_stdv) for x in env.evaluation_result_list]
            if env.iteration == 0 and fold == 0:
                score_names_list = [score.split(':')[0] for score in result_list]
                score_names = '\t'.join([name for name in score_names_list])
                logger.log(level, 'fold\titeration\t{}'.format(score_names))
            scores_list = [score.split(':')[1] for score in result_list]
            scores = '\t'.join([score for score in scores_list])
            logger.log(level, '{}\t{}\t{}'.format(fold, env.iteration + 1, scores))
    _callback.order = 10
    return _callback
