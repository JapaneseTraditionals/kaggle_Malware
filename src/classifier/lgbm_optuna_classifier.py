import gc
from pathlib import Path
import numpy as np
import pandas as pd
import lightgbm as lgb
import optuna
from logging import getLogger
from save_log import get_version
from clf_util import fast_auc, eval_auc
from classifier.base_classifier import BaseClassifier
from classifier.lgbm_classifier import LGBMClassifier
from save_log import stop_watch, get_back_training, train_one_round, send_message


class LGBMOptunaClassifier(BaseClassifier):

    def objective(self, trial):
        # Extract optuna attribs from the input json
        optuna_trn_params = {}
        for key, val in self.params["trn_params"].items():
            if type(val) != list:
                optuna_trn_params[key] = val
            else:
                if type(val[0]) == float:
                    optuna_trn_params[key] = trial.suggest_uniform(key, val[0], val[1])
                elif type(val[0]) == int:
                    optuna_trn_params[key] = trial.suggest_int(key, val[0], val[1])
                else:
                    optuna_trn_params[key] = trial.suggest_categorical(key, val)

        # Initialize parameters
        mtd_params = self.params["mtd_params"]
        validity = None
        model_path = Path(__file__).absolute().parents[2] / "data" / "model" / str(get_version())
        Path.mkdir(model_path, exist_ok=True, parents=True)
        START_FOLD = 0
        if get_back_training():
            START_FOLD = len(list(model_path.glob('**/*.model')))
        END_FOLD = 5
        if train_one_round():
            START_FOLD = 0
            END_FOLD = 1
        if START_FOLD == END_FOLD:
            return None

        # Process for each fold
        for fold in range(START_FOLD, END_FOLD):
            print("\t [OPTUNA] >> {} folds start".format(fold))
            send_message("\t :sushi: {} folds start".format(fold))

            # Generate dataset
            valid = "valid{}".format(str(fold))
            trn_x = super().get_feature_df(self.feature_names, valid, "train")
            val_x = super().get_feature_df(self.feature_names, valid, "validate")
            trn_x.set_index("MachineIdentifier", inplace=True)
            val_x.set_index("MachineIdentifier", inplace=True)
            trn_y = trn_x["HasDetections"].astype(np.int8)
            val_y = val_x["HasDetections"].astype(np.int8)
            train_dataset = lgb.Dataset(trn_x, trn_y)
            valid_dataset = lgb.Dataset(val_x, val_y)

            # Initialize variables for scoring
            if validity is None:
                validity = pd.DataFrame()
                validity["HasDetections"] = pd.concat([trn_y, val_y])
                validity["Predict"] = 0

            # Delete needless features
            del trn_x["HasDetections"], val_x["HasDetections"]

            # Classify
            clf = lgb.train(optuna_trn_params,
                            train_dataset,
                            mtd_params["num_boost_round"],
                            valid_sets=[train_dataset, valid_dataset],
                            feval=eval_auc,
                            verbose_eval=mtd_params["verbose_eval"],
                            early_stopping_rounds=mtd_params["early_stopping_rounds"])
            validity.loc[validity.index.isin(val_x.index), "Predict"] = clf.predict(val_x, num_iteration=clf.best_iteration)

            # Post-process this fold
            del train_dataset, valid_dataset
            gc.collect()
            print("\t [OPTUNA] >> {} folds end".format(fold))

        # Output CV score
        validity = validity.reset_index()
        columns_order = ["MachineIdentifier", "HasDetections", "Predict"]
        validity = validity.sort_values("MachineIdentifier").reset_index(drop=True).loc[:, columns_order]
        cv_auc = (fast_auc(validity["HasDetections"], np.array(validity["Predict"])))

        return cv_auc

    @stop_watch("LGBMOptunaClassifier.train()")
    def train(self, feature_names):
        self.feature_names = feature_names
        study = optuna.create_study()
        study.optimize(self.objective, n_trials=self.params["n_trials"])
        best_trn_params = study.best_params
        getLogger(get_version()).info("\t >> Best params: {}".format(best_trn_params))

        for key, val in self.params["trn_params"].items():
            if type(val) != list:
                best_trn_params[key] = val
        self.params["trn_params"] = best_trn_params

        self.best_lgbm_classifier = LGBMClassifier(self.params, self.dataset_name)
        return self.best_lgbm_classifier.train(feature_names)

    @stop_watch("LGBMClassifier.predict()")
    def predict(self, feature_names):
        return self.best_lgbm_classifier.predict(feature_names)
