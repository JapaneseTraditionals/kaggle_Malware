import pandas as pd
import numpy as np
import lightgbm as lgb
import dask.dataframe as dd
from scipy.sparse import vstack, csr_matrix, save_npz, load_npz
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from slackclient import SlackClient
from pathlib import Path
import gc
gc.enable()
ROOT_PATH = Path(".").absolute()

VERSION = str(Path(__file__)).split('/')[-1].split('.')[0]
SUBMIT_FILE = "{}.csv".format(VERSION)

max_observations = 1000
min_threshold_for_unbalanced_values = 0.20
max_threshold_for_unbalanced_values = 0.80
random_state = 16

n_estimators = 10
learning_rate = 0.1
num_leaves = 128
min_child_samples = 240
colsample_bytree = 0.28
early_stopping_rounds = 1000

usecols = {
    'MachineIdentifier',
    'EngineVersion',
    'AppVersion',
    'AvSigVersion',
    'IsBeta',
    'RtpStateBitfield'
}

dtypes = {
    'MachineIdentifier'                                 : 'category',
    'EngineVersion'                                     : 'category',
    'AppVersion'                                        : 'category',
    'AvSigVersion'                                      : 'category',
    'IsBeta'                                            : 'int8',
    'RtpStateBitfield'                                  : 'float16',
    'HasDetections'                                     : 'int8'
}


def read_token(token_path):
    token = ""
    if token_path.exists() is not True:
        return None
    with open(token_path, 'r') as f:
        token = f.read()
    token = token.replace('\n', '')
    return token

CHANNEL = "log"
TOKEN_FILE = ".slack_token"
TOKEN_PATH = Path(__file__).absolute().parents[2] / TOKEN_FILE

token = read_token(TOKEN_PATH)
client = SlackClient(token)


def send_message(text):
    text = "[{}]: {}".format(SUBMIT_FILE, text)
    client.api_call(
        "chat.postMessage",
        channel=CHANNEL,
        text=text
    )


send_message("Reading test.csv")
print("Reading test.csv")
test = dd.read_csv(ROOT_PATH / "input" / "test.csv", dtype=dtypes, usecols=usecols).compute()
test['MachineIdentifier'] = test.index.astype('uint32')
usecols.add("HasDetections")

send_message("Reading train.csv")
print("Reading train.csv")
train = dd.read_csv(ROOT_PATH / "input" / "train.csv", dtype=dtypes, usecols=usecols).compute()
train['MachineIdentifier'] = train.index.astype('uint32')


send_message("Transforming all features to category.")
print("Transforming all features to category.")
for usecol in train.columns.tolist()[1:-1]:

    train[usecol] = train[usecol].astype('str')
    test[usecol] = test[usecol].astype('str')

    le = LabelEncoder().fit(
        np.unique(train[usecol].unique().tolist() + test[usecol].unique().tolist()))

    train[usecol] = le.transform(train[usecol]) + 1
    test[usecol] = le.transform(test[usecol]) + 1

    agg_tr = (train
              .groupby([usecol])
              .aggregate({'MachineIdentifier': 'count'})
              .reset_index()
              .rename({'MachineIdentifier': 'Train'}, axis=1))
    agg_te = (test
              .groupby([usecol])
              .aggregate({'MachineIdentifier': 'count'})
              .reset_index()
              .rename({'MachineIdentifier': 'Test'}, axis=1))

    agg = pd.merge(agg_tr, agg_te, on=usecol, how='outer').replace(np.nan, 0)
    agg = agg[(agg['Train'] > max_observations)].reset_index(drop=True)
    agg['Total'] = agg['Train'] + agg['Test']
    agg = agg[(agg['Train'] / agg['Total'] > min_threshold_for_unbalanced_values) & (agg['Train'] / agg['Total'] < max_threshold_for_unbalanced_values)]
    agg[usecol + 'Copy'] = agg[usecol]

    train[usecol] = (pd.merge(train[[usecol]],
                              agg[[usecol, usecol + 'Copy']],
                              on=usecol, how='left')[usecol + 'Copy']
                     .replace(np.nan, 0).astype('int').astype('category'))

    test[usecol] = (pd.merge(test[[usecol]],
                             agg[[usecol, usecol + 'Copy']],
                             on=usecol, how='left')[usecol + 'Copy']
                    .replace(np.nan, 0).astype('int').astype('category'))

    del le, agg_tr, agg_te, agg, usecol
    gc.collect()

y_train = np.array(train['HasDetections'])
train_ids = train.index
test_ids = test.index

del train['HasDetections'], train['MachineIdentifier'], test['MachineIdentifier']
gc.collect()

send_message("Fitting features with OneHotEncoder")
print("Fitting features with OneHotEncoder")
ohe = OneHotEncoder(categories='auto', sparse=True, dtype='uint8').fit(train)

m = 100000
train = vstack([ohe.transform(train[i * m:(i + 1) * m]) for i in range(train.shape[0] // m + 1)])
test = vstack([ohe.transform(test[i * m:(i + 1) * m]) for i in range(test.shape[0] // m + 1)])
save_npz(str(ROOT_PATH / "tmp" / 'train.npz'), train, compressed=True)
save_npz(str(ROOT_PATH / "tmp" / 'test.npz'), test, compressed=True)

del ohe, train, test
gc.collect()

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)
skf.get_n_splits(train_ids, y_train)

lgb_test_result = np.zeros(test_ids.shape[0])
counter = 0

send_message("Starting the classification...")
print("Starting the classification...")
for train_index, test_index in skf.split(train_ids, y_train):
    send_message("Fold {}\n".format(counter + 1))
    print('Fold {}\n'.format(counter + 1))

    train = load_npz(str(ROOT_PATH / "tmp" / 'train.npz'))
    X_fit = vstack([train[train_index[i * m:(i + 1) * m]] for i in range(train_index.shape[0] // m + 1)])
    X_val = vstack([train[test_index[i * m:(i + 1) * m]] for i in range(test_index.shape[0] // m + 1)])
    X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')
    y_fit, y_val = y_train[train_index], y_train[test_index]

    del train
    gc.collect()

    lgb_model = lgb.LGBMClassifier(max_depth=-1,
                                   n_estimators=n_estimators,
                                   learning_rate=learning_rate,
                                   num_leaves=num_leaves,
                                   min_child_samples=min_child_samples,
                                   colsample_bytree=colsample_bytree,
                                   objective='binary',
                                   n_jobs=-1)

    lgb_model.fit(X_fit, y_fit, eval_metric='auc',
                  eval_set=[(X_val, y_val)],
                  verbose=100, early_stopping_rounds=early_stopping_rounds)

    del X_fit, X_val, y_fit, y_val, train_index, test_index
    gc.collect()

    test = load_npz(str(ROOT_PATH / "tmp" / 'test.npz'))
    test = csr_matrix(test, dtype='float32')
    lgb_test_result += lgb_model.predict_proba(test)[:, 1]
    counter += 1

    del test
    gc.collect()

send_message("Generating the sample submission...")
print("Generating the sample submission...")

submission = pd.read_csv(str(ROOT_PATH / "input" / "sample_submission.csv"))
submission['HasDetections'] = lgb_test_result / counter
submission.to_csv(ROOT_PATH / "data" / "submit" / "lgb_submission3.csv", index=False)

send_message("Done.")
print("Done.")
